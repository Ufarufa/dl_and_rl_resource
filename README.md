# 딥러닝 & 강화 학습 관련 자료들.
###Python & tensorflow
- [intsatll tensorflow in ubuntu16](http://www.popit.kr/tensorflow-install-ubuntu16/)
- [python tutorial 5hours](https://www.youtube.com/watch?v=emY34tSKXc4)
- [python study by MS](https://mva.microsoft.com/ko/training-courses/python%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%9C-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EC%86%8C%EA%B0%9C-8360?l=CrrhO0O8_6204984382)
- [python 이해하기](http://www.slideshare.net/dahlmoon/python-20160815)
- [Node.js by MS](https://mva.microsoft.com/en-US/training-courses/using-nodejs-with-visual-studio-code-13920?l=nSEpCdzbB_001937557)
- [neural-networks-and-tensorflow-introduction](https://tensorflowkorea.wordpress.com/2016/08/23/dl-with-neural-networks-and-tensorflow-introduction/)
- [R에서 파이썬까지…데이터과학 학습 사이트 8곳](http://www.bloter.net/archives/237013?rccode=lvRc)

[유사 논문 검색기](http://www.arxiv-sanity.com/top)

###Daily Check
- https://twitter.com/DeepMindAI
- https://twitter.com/demishassabis
- https://twitter.com/OpenAI 
- https://twitter.com/DeepLearningHub
- https://twitter.com/jackclarksf
- https://twitter.com/karpathy
- [논문 설명]https://github.com/karpathy/paper-notes

## Deep Learning
###Algorithm
- CNN
  - GoogLe Net ( using InceptionV3 )

- Recurrent Network
  - LSTM
  - RLU
  - E2E ( end to end Memory )
  - [generative-models](https://openai.com/blog/generative-models/)
  
###Project
- Magenta 
  - [Google Magenta(작곡 딥러닝)](https://tensorflowkorea.wordpress.com/2016/07/11/magentas-paper-reviews/)
  - [music-art-and-machine-intelligence](https://tensorflowkorea.wordpress.com/2016/07/17/music-art-and-machine-intelligence-workshop-2016/)
  
### DataSet
- [3D Human Pose DataSet](http://domedb.perception.cs.cmu.edu/dataset.html)
- [google youtube video dataSet](https://research.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html)
- [google image dataSet](https://tensorflowkorea.wordpress.com/2016/10/02/open-images-dataset/)
- [KAIST_Corpus](http://semanticweb.kaist.ac.kr/home/index.php/KAIST_Corpus)

###Posting
- [openai-infrastructure-for-deep-learning](https://openai.com/blog/infrastructure-for-deep-learning/)
- [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/)
- [how-to-run-text-summarization-with-TF](https://medium.com/@surmenok/how-to-run-text-summarization-with-tensorflow-d4472587602d#.4cec2w7t3)
- [WaveNet: DeepMind’s New Model for Audio](https://tensorflowkorea.wordpress.com/2016/09/09/wavenet-deepminds-new-model-for-audio/)
- [Generating Videos with Scene Dynamics](http://web.mit.edu/vondrick/tinyvideo/)
- [Deep Learning For 이미지 분석 ](https://www.facebook.com/groups/TensorFlowKR/permalink/321214624886269/) 
- [Image Completion with Deep Learning in TensorFlow](http://bamos.github.io/2016/08/09/deep-completion/)
  - [구현](https://github.com/jazzsaxmafia/Inpainting)   
- [Face Auto Gen](https://tensorflowkorea.wordpress.com/2016/08/18/neural-facegrid-by-discgen/)
- [Google opensource new image captioning](https://tensorflowkorea.wordpress.com/2016/09/23/google-opensource-new-image-captioning-model-im2txt/)
- [ConvNet with WebGL](https://tensorflowkorea.wordpress.com/2016/08/18/convnet-with-webgl/)
- deeplearning2016_montreal
  - [Video](http://videolectures.net/deeplearning2016_montreal/)
- [AI 온라인 컨퍼런스](http://ai.withthebest.com/)
- [Deep Learing Book Study](https://tensorflowkorea.wordpress.com/2016/09/23/deep-learning-textbook-study-group/)
- [Machine Learning Top 10 Articles ](https://medium.mybridge.co/machine-learning-top-10-articles-for-the-past-month-2f3cb815ffed#.v5pj4aa7o)
- [Beyond Deep Learning – 3rd Generation Neural Nets](http://www.datasciencecentral.com/profiles/blogs/beyond-deep-learning-3rd-generation-neural-nets)
- facebook Object Recognation in Image 
  - [facebook posting](https://code.facebook.com/posts/561187904071636)
  - [tensorflowkorea Po](https://tensorflowkorea.wordpress.com/2016/08/26/facebook-open-source-image-recognition-tools/)
  
###tensorflow
- [distributed-tensorflow](https://tensorflowkorea.wordpress.com/2016/07/17/distributed-tensorflow-design-patterns-and-best-practices/)

###Lecture
- sung kim - https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm
- CS231n: [Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) / [Video](https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC) /[강의 노트 한글](http://ishuca.tistory.com/category/CS231n)
- Kmooc :
  - 경영데이터 마이닝 : http://www.kmooc.kr/courses/course-v1:HYUk+HYUBUS3099k+2016_C1/info
  - 인공지능과 기계학습 : http://www.kmooc.kr/courses/course-v1:KAISTk+KCS470+2015_K0201/info
- [Deep Generative Models](https://portal.klewel.com/watch/webcast/deep-learning-tools-and-methods-workshop/talk/6)
- [ICML’15](http://dpkingma.com/?page_id=483)
- [Google I/O Extended Seoul 2016 - Tensorflow 101](https://www.youtube.com/watch?v=7UwAz4Jvvko)
- [Deep Learning for Computer Vision Barcelona 2016 ](http://imatge-upc.github.io/telecombcn-2016-dlcv/)
- [코세라 NLP](https://www.coursera.org/learn/natural-language-processing)
- [아카데믹 토렌트](http://academictorrents.com/)
- [
creative-applications-of-deep-learning-with-TF](https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow/info)
- EXEM Seminar
  - [뇌과학으로 본 인공지능의 현주소와 미래 01](https://www.youtube.com/watch?v=efWSbITntR0)
  - [EXEM Seminar / 뇌과학으로 본 인공지능의 현주소와 미래 ](https://www.youtube.com/watch?v=48EsevSvoRw)

 
###Papers
- [OpenAI-Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498v1.pdf)
- [Video Pixel Networks](https://arxiv.org/pdf/1610.00527v1.pdf)
- A New Method to Visualize Deep Neural Networks( Deep NN  시각화 ) -  http://arxiv.org/abs/1603.02518v2
- Deep Speech 2(음성인식) - https://arxiv.org/abs/1512.02595
- [Bag of Tricks for Efficient Text Classification-CPU가 더 빠른.](https://arxiv.org/pdf/1607.01759v2.pdf)
- [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](https://arxiv.org/abs/1606.01847)
- [지워진 부분 그리기 Context Encoders: Feature Learning by Inpainting](http://people.eecs.berkeley.edu/~pathak/context_encoder/)
- [Layer Normalization](http://arxiv.org/pdf/1607.06450.pdf) - 배치 노말라이제이션을 변형하여 입력 데이터의 평균과 분산을 이용해 레이어 노말라이제이션을 적용
  - https://tensorflowkorea.wordpress.com/2016/07/24/layer-normalization/ 
- [Matching Networks for One Shot Learning  ](https://arxiv.org/pdf/1606.04080.pdf)
  - [정리](https://github.com/karpathy/paper-notes/blob/master/matching_networks.md)
- [학습과 에러전파를 따로-DeepMind Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/pdf/1608.05343.pdf) 
  - [summary](https://tensorflowkorea.wordpress.com/2016/08/22/decoupled-neural-interfaces-using-synthetic-gradients1608-05343-summary/) 
  - [Deep mind BLog](https://deepmind.com/blog#decoupled-neural-interfaces-using-synthetic-gradients)
  - [summary2](https://tensorflowkorea.wordpress.com/2016/08/31/synthetic-gradient-revisited/)
- [Densely Connected Convolutional Networks](http://arxiv.org/abs/1608.06993)
  - 오늘 소개시켜 드릴 논문은 "Densely Connected Convolutional Networks" (http://arxiv.org/abs/1608.06993)입니다.
일종의 residual network라고 볼 수 있을거 같습니다. 그런데 논문에 나온 그래도, "each layer is directly connected to every other layer in a feed-forward fashion" 모든 레이어가 앞에 나오는 모든 레이어와 직접 연결되어 있습니다. 그래서 이름이 Densely Connected Convolutional Networks (DenseNet) 입니다.
일단 설명만 들어도 복잡하고 파라미터가 많아질거 같습니다. 그러나 저자들은 이렇게 연결을 많이 해서 생기는 장점때문에 오히려 적은 레이어가 필요하여 생각하는 만큼 오버헤드도 크지 않고 오히려 효율적이라고 하고 있습니다. 그 중 가장 중점이 되는 주장은 skip-connection으로 모든 레이어에 연결되어 있어 feature의 재상용이 증가하고, 그럼으로 파라미터가 줄어든다고 합니다.
음.. 저자들의 일방적인 주장같지만 어쩌면 일리가 있습니다. Residual Networks are Exponential Ensembles of Relatively Shallow Networks(https://arxiv.org/abs/1605.06431)에 보면 어려운 task를 할 때 resinet을 이루는 Shallow Networks들의 길이가 더 길어지고 쉬운 일을 할 때 좀 짧아진다고 한듯...(기억이 가물..가물) 합니다. 그렇다면 중복이나 비효율성이 있을거고 그것을 줄이는 방법으로 한듯 합니다.
이렇게 무식해 보이지만 다 연결해서 몇 public set에서는 최고 성능을 찍었습니다. 그러고 보니 최근 이런 식의 skip connection을 늘리는 연구가 많이 진행 되었습니다.
    - AdaNet: Adaptive Structural Learning of Artificial Neural Networks (https://arxiv.org/pdf/1607.01097v1.pdf)
    - Collaborative Layer-wise Discriminative Learning in Deep Neural Networks (https://arxiv.org/pdf/1607.05440v1.pdf)
AdaNet이나 Collaborative Layer-wise Discriminative Learning 같은 경우는 skip connection을 상황에 맞게 늘리는 것으로 이해했습니다. (물론 제가 맞는지 저도 모른다는거..) 비교할만한 성능이 제시 안 되어 있어 비교는 힘들지만, deep learning이라는 점에서 무식해 보이는 densenet이 오히려 좋을 수 있을거라는 생각이 듭니다. 학습 과정에서 또 알아서 최적화를 해 버릴 수 있다는 이상한 예감이....
 
###구현 
- deep-learning-for-chatbots : http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
- [Caption Gen, slide Shaer](http://www.slideshare.net/ssuser06e0c5/a-neural-image-caption-generator)
- [Papers implemented By Tensorflow](https://github.com/LeavesBreathe/tensorflow_with_latest_papers)
- [fast waveNet 구현](https://github.com/tomlepaine/fast-wavenet)




