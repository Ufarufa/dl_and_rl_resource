##Reinforcement Learning

###Algorithm
- DQN ( Deep Q-Learing )
- Q-learning
- Policy Gradient
- A3C ( Asynchronous Advanced Actor-Critic )
- MDP (Markov Decision process)
- OO-MDP (Object-oriented Markov Decision Process)
- Gym

###Lecture & Books
- [The Nuts and Bolts of Deep RL Research-NIPS2016- Schulman](http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf)
- [CS188 2013](https://www.youtube.com/user/CS188Spring2013)
- [CS188 2014](https://www.youtube.com/watch?v=IXuHxkpO5E8)
- [CS188 2016](https://www.youtube.com/watch?v=3aCn2-Slaoc&list=PLIeooNSdhQE5kRrB71yu5yP9BRCJCSbMt)
- David Sliver (Deep Mind)
  - [DeepMind Blog](https://deepmind.com/blog/deep-reinforcement-learning/)
- [John Schulman](https://www.youtube.com/watch?v=aUrX-rP_ss4&index=1&list=PLUdqnrHmtdXQuivfwPyEKi3y5eQJqo1c4)
- Udacity - Reinforcement Learning
- [sutton book 2015 ](https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf?dl=0)
- Replicating Deep Q Learning with TensorFlow 
  - [1](https://www.youtube.com/watch?v=suNNrEHDR-I) / [2](https://www.youtube.com/watch?v=Zu-oNPPVvfI)
- [Policy Gradients wiki](http://www.scholarpedia.org/article/Policy_gradient_methods)
- [모두의 연구소 RL 책](https://dnddnjs.gitbooks.io/rl/content/)
- [RL 기초 ](https://norman3.github.io/rl/)
- [인공지능 By SNU](https://www.youtube.com/playlist?list=PLzWH6Ydh35ggVGbBh48TNs635gv2nxkFI)
- [Introduction to Markov Chain Monte Carlo](http://www.mcmchandbook.net/HandbookChapter1.pdf)
- [LEARNING REINFORCEMENT LEARNING By Wide ML](http://www.wildml.com/2016/10/learning-reinforcement-learning/#comments)
- Hasbiss in DeppMind
  - [Hasbiss autumn Lecture](https://www.youtube.com/watch?v=Psk5DLpqp3o)
  - [Demis Hassabis discusses his vision ](https://www.youtube.com/watch?v=iHrn2zLFh8Y&index=41&list=WL)

### Posting
- [자바스크립트로 만드는 객체지향형 인공지능]https://www.facebook.com/groups/132184990601054/permalink/198511323968420/
- [DEEP REINFORCEMENT LEARNING: AN OVERVIEW](https://arxiv.org/pdf/1701.07274.pdf)
- [Introduction to Q-Learning](https://medium.com/@aneekdas/introduction-to-q-learning-88d1c4f2b49c#.w1at07ivb)
- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html)
- [AI Begins to Understand the 3-D World](https://www.technologyreview.com/s/603107/ai-begins-to-understand-the-3-d-world/)
- [딥러닝과 강화 학습으로 나보다 잘하는 쿠키런 AI 구현하기-동강](http://serviceapi.rmcnmv.naver.com/flash/outKeyPlayer.nhn?vid=744982879C4B8D95A768185158158FA1F14E&outKey=V122a39972b8120461b33134a56dad62b1db97657ca107701108d134a56dad62b1db9&controlBarMovable=true&jsCallable=true&skinName=tvcast_white)
- [awesome-starcraftAI](https://github.com/SKTBrain/awesome-starcraftAI)
- Deep Mind blog
    - [Enabling Continual Learning in Neural Networks](https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/)
    - [blog](https://deepmind.com/blog)
    - [reinforcement-learning-unsupervised-auxiliary-tasks](https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/)
        - [기사](http://m.news.naver.com/read.nhn?oid=092&aid=0002106658&sid1=105&mode=LSD)
        - 현재 머신러닝은 가공을 거친 데이터를 이용해 학습하는 수퍼바이즈드 러닝, 즉 지도 학습이 주류다. 바둑이 머신러닝과 강화학습에 적합한 것은 19X19라는 정해진 틀 안에서 모든 사건이 이뤄져 데이터의 후가공이 편하기 때문이다. 따라서 수퍼바이즈드 러닝을 쉽게 적용할 수 있다. 바둑 뿐 아니라 장기, 체스와 같은 보드 게임은 이같은 측면에서 모두 비슷하다. 딥마인드가 알파고 전에 테스트한 아타리 게임들도 대부분 한 화면 내에서 이뤄지는 게임들이다.
여기에 딥마인드가 하나를 더 보탰다. 기존 아타리 게임 에이전트에 보조 작업(auxiliary tasks)을 추가해 3D 게임과 같이 언수퍼바이즈드 러닝, 즉 비지도 학습에 가까운 게임에도 적용할 수 있도록 한 것이다. 이 에이전트의 이름은 언리얼(UNREAL, UNsupervised REinforcement and Auxiliary Learning)로 이전 에이전트보다 학습 능력이 무려 10배 이상 높아져 고수들의 결과의 87%까지 도달할 수 있었다고 한다. 몇몇 스테이지에서는 사람보다도 더 높은 성적을 올렸다. 다른 아타리 게임에서의 성능도 향상돼 사람보다 9배 높은 점수를 따냈다.
기계학습 부문에서 향후 비지도 학습, 즉 데이터 가공 없이 바로 현장에서 취합하는 데이터를 이용해 학습을 진행할 수 있도록 하는 분야가 각광을 받을 것이라고 들은 바 있다. 아직 연구를 많이 해야 하고 그만큼 가능성도 높다고 한다. 딥마인드가 바로 이같은 비지도학습에서도 한걸음 나아간 것으로 보인다.
    - [Deep Mind-Differentiable neural computers- 기학습된 네트워크에서 내용 가져와서 다른 학습](https://deepmind.com/blog/differentiable-neural-computers/)
        - [korean Discription](https://tensorflowkorea.wordpress.com/2016/10/13/deepminds-new-paper-memory-augmented-neural-network/)
        - [impl](https://github.com/Mostafa-Samir/DNC-tensorflow)
- [Deep Reinforcement Learning: Playing a Racing Game](https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html)
- [Deep Reinforcement Learning for Driving Policy](https://www.youtube.com/watch?v=QK0LxA8FWq4&feature=share)
- [Open Ai 분석-모두의 연구소](http://www.modulabs.co.kr/RL_library/2621)
- [딥 강화학습 쉽게 이해하기](http://ddanggle.github.io/ml/ai/cs/2016/09/24/demystifyingDL.html)
- [Replicating Deep Q Learning with TensorFlow - 한국어](https://www.youtube.com/watch?v=suNNrEHDR-I)
- [시리즈]Simple Reinforcement Learning with Tensorflow
    - [Part 0: Q-Learning with Tables and Neural Networks](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.6bd9pfsdv) 
- [Simple reinforcement learning ](http://kvfrans.com/simple-algoritms-for-solving-cartpole/)
- [GP By karpathy](http://karpathy.github.io/2016/05/31/rl/)
- [Deep Reinforcement Learning: Frontiers and Challenges, IJCAI 2016](https://sites.google.com/site/deeprlijcai16/programme)
- [dueling_dqn](http://torch.ch/blog/2016/04/30/dueling_dqn.html)
- [torch blog](http://torch.ch/blog/index.html)
- [텐서플로우(TensorFlow)를 이용해 간단한 DQN(Deep-Q-Networks) ](http://solarisailab.com/archives/486)
- [Reinforce Pong at Gym-tensorflowKorea](https://tensorflowkorea.wordpress.com/2016/07/13/reinforce-pong-at-gym/)
- [Q-learing + CNN를 이용한 지역화](http://www.slideshare.net/ssuser06e0c5/q-learning-cnn-object-localization)
- [Semantic representations in the temporal pole predict false memories](http://www.pnas.org/content/early/2016/08/16/1610686113.long)
- [Deep Deterministic Policy Gradients in TensorFlow](http://pemami4911.github.io/blog_posts/2016/08/21/ddpg-rl.html)
- [rl4j-DQN](https://rubenfiszel.github.io/posts/rl4j/2016-09-08-DQN-Learning-to-play-from-pixels-step-by-step.html)
- [Teaching Your Computer To Play Super Mario Bros](http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/?ref=mybridge.co)
- [튜토리얼 ICML2016-DL](http://techtalks.tv/talks/deep-reinforcement-learning/62360/)
- [deep-reinforcement-learning-with-openai](http://www.rubedo.com.br/2016/08/deep-reinforcement-learning-with-openai.html)
- Reinforment L 
  - [1. Demystifying Deep Reinforcement Learning](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/)
  - [2. deep-reinforcement-learning-with-neon](https://www.nervanasys.com/deep-reinforcement-learning-with-neon/)
  - [3. Deep Reinforcement Learning with OpenAI Gym](https://www.nervanasys.com/openai/)

###conference
- [NIPS-2016]
  - [Deep Reinforcement Learning Workshop, NIPS 2016](https://sites.google.com/site/deeprlnips2016/)
  - [Generative Adversarial Networks (GANs) tutorial](http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf)
  - [Deep	Reinforcement	Learning through Policy Optimiztion-Jhon schulman](http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf)
  - [NIPS 2016 Generative Adversarial Training workshop talk](http://www.nowozin.net/sebastian/blog/nips-2016-generative-adversarial-training-workshop-talk.html)
  - [NIPS-KeyNotes Yann Le Cun](https://drive.google.com/file/d/0BxKBnD5y2M8NREZod0tVdW5FLTQ/view)
    - I made the following main points:  
>1. supervised learning works well if you have lots of labeled data  
>2. But it's insufficient if we want machines to learn enough about how the world works to acquire "common sense"  
>3. The only way for machines to acquire common sense is to observe the world and act in it, like humans and animals.  
>4. To become more intelligent, machines will need to learn predictive models of the world.   
>5. Entity RNN is a new model that maintains an estimate of the state of the world from textual descriptions of events.  
>6. It is the only model that can solve all 20 of the bAbI tasks with good accuracy.  
>7. predictive world models can be used for intelligent planning and learning by playing out actions sequences and simulating the result without actually affecting the world.  
>8. But they also need to learn in a goal-directed fashion. This is a plea for model-based reinforcement learning.  
>9. Predictive learning can be formulated in an energy-based framework. It comes down to learning to predict with uncertainty.  
>10. The most promising idea for learning under uncertainty is adversarial training.  
>11. energy-based generative adversarial networks can synthesize high-resolution images.  
>12. adversarial training enables the training of video prediction systems and get around the "blurry prediction" problem of least-square criteria.  

###Environment
- [Universe: Measurement and training for AI-openAI](https://tensorflow.blog/2016/12/05/universe-measurement-and-training-for-ai/)
- [Open-sourcing DeepMind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/)
- [Environment disc By Yann LeCun]
  - Three companies are announcing three different environments that use games and virtual environments for AI research:  
TorchCraft (Facebook): https://github.com/TorchCraft/TorchCraft  
DeepMind Lab (DeepMind): https://deepmind.com/blog/open-sourcing-deepmind-lab/  
OpenAI Universe (OpenAI): https://universe.openai.com/  
Why is everyone so interested in games?  
Well, few AI researchers are *actually* interested in games from the professional standpoint (with the notable exception of my dear colleague Julian Togelius !).  
But one of the big obstacles to progress in AI is the construction and testing of complete AI agents that perceive, reason, remember, and act in complex environments.  
AI systems must be able to model their environment, predict the consequence of their actions, identify causal relationships so they can act in ways that will influence the environment.  
In my opinion, designing methods that will allow AI systems to learn good predictive models of the world in an unsupervised manner is one of the biggest challenges of AI research today.  
AI systems in complex environments also need to have basic drives and motivations that will make them do what they are supposed to do in a safe and efficient way. This can be encoded in objective functions that provide intrinsic motivations for AI systems.  
With a predictive world model and an objective function that provides intrinsic motivations, an AI agent also needs to be able to produce action sequences that will make the environment reach a state that optimizes the objective. This is the problem of planning under uncertainty.  
To perform long-term planning, an AI agent needs to be able to decompose a complex task into subtasks, or replace a long-term goal by a series of sub-goals, or replace a long-term objective by a series short-term surrogate objectives.  
Certain games provide good environment for developing, experimenting and testing AI methods and principles to solve all the above problems.  
Of course, one could also use physical robots and real-world tasks to develop these techniques, but progress would be slow because there are two major problems with the real world:  
>1. it is difficult to reproduce results, and more importantly...  
>2. The main problem with the real world is that you can't run it faster than real time.  
So, AI research labs do not work on games because they are frivolous. They work on games because the virtual environments provided by games give a perfect playground in which to make fast and measurable progress in unsupervised learning, reasoning, hierarchical planning and reasoning, causal inference, and a host of other problems.  
In parallel, the methods developed in the context of virtual environments, combined with other research in computer perception, natural language processing, theoretical insights, and algorithmic advances, will enable us to develop dialogue agents, intelligent virtual assistants, all-knowing question-answering systems, and intelligent robots.  

###Paper
- [*FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING](https://arxiv.org/pdf/1611.02779.pdf)
- Learn Mario playing : Evolving Neural Networks through Augmenting Topologies - http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf
- Playing Atari : Playing Atari with Deep Reinforcement Learning(DQN) - http://arxiv.org/abs/1312.5602
- Play FlappyBird( DQN imp ) - http://sarvagyavaish.github.io/FlappyBirdRL/
- [Dueling Network Architectures for Deep Reinforcement Learning](http://arxiv.org/pdf/1511.06581v3.pdf)
- [Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks](http://arxiv.org/abs/1609.02993)
- [Multi-Objective Deep Reinforcement Learning](https://arxiv.org/abs/1610.02707)
- [Towards Deep Symbolic Reinforcement Learning](https://arxiv.org/pdf/1609.05518.pdf)
- DeepMind Papers : https://deepmind.com/publications.html
  - [*Overcoming catastrophic forgetting in neural networks](http://www.pnas.org/content/early/2017/03/13/1611835114.full)
    - desc : https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/
  - [*Neural Episodic Control->Rapid deep reinforcement learning using neural dictionaries](https://arxiv.org/pdf/1703.01988.pdf)
  - [*Count-Based Exploration with Neural Density Models->PixelCNN-based Intrinsic Motivation improves exploration in the hardest Atari games](https://arxiv.org/abs/1703.01310)
  - [FeUdal Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1703.01161)
  - [Understanding Synthetic Gradients and Decoupled Neural Interfaces](https://arxiv.org/abs/1703.00522)
  - [Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees](https://arxiv.org/abs/1702.08833)
  - [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)
    - [imp in tensorFlow](https://github.com/deepmind/learning-to-learn)
  - [DeepMind-Nips2016 papers]
    - [part1](https://deepmind.com/blog/deepmind-papers-nips-part-1/)
    - [part2](https://deepmind.com/blog/deepmind-papers-nips-part-2/)
    - [part3](https://deepmind.com/blog/deepmind-papers-nips-part-3/)
  - [LEARNING TO REINFORCEMENT LEARN](https://arxiv.org/pdf/1611.05763v1.pdf)
  - [ Agent의 내적 동인으로 '통제감'을 만들어 넣어 보여주고-Variational Intrinsic Control](https://arxiv.org/abs/1611.07507)
    - DeepMind의 새 AI 논문입니다. Agent의 내적 동인으로 '통제감'을 만들어 넣어 보여주고 있습니다. 영화 등에서 다루는 두려운 존재로서의 AI가 탄생 가능할 것 같다는 생각이 드는... 연구네요.
(인간의 내적 동인 이론인 SDT(http://selfdeterminationtheory.org/) 에서는 주 동인으로 Autonomy, Mastery, Relatedness를 열거하고 있습니다. 이 중에서, 이번 연구는 agent가 autonomy, mastery를 추구하도록 하는 것으로 보입니다.)
  - [DeepMind-Early Visual Concept Learning with Unsupervised Deep Learning](https://arxiv.org/pdf/1606.05579.pdf)
  - [DeepMind-Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/pdf/1606.05579.pdf)
  - [DeepMind-Asynchronous Methods for Deep Reinforcement Learning](http://arxiv.org/pdf/1602.01783.pdf)
  - [DeepMind-Continuous Deep q-Learning with Model-based Acceleration](http://arxiv.org/pdf/1603.00748v1.pdf)
    - [구현](https://github.com/carpedm20/NAF-tensorflow)
    - 논문에서 비교하고 있는 Deep Deterministic Policy Gradient (DDPG)는 Continuous space에서 actor-critic을 사용해서 문제를 해결했는데, 이 논문에서는 duelling network와 유사한 구조를 사용해서 적은 파라미터와 비교적 간단한 알고리즘으로 학습이 가능하다고 합니다. OpenAI에서 공개한 Requests for Research 중에서 하나가 이 논문을 구현하는건데, 이제 hyperparameter를 바꾸지 않고 얼마나 많은 문제를 풀 수 있을지 실험을 해 봐야할 것 같네요.
    최근에 supervised learning 문제에 대한 discussion은 많이 보이던데, DeepMind나 OpenAI가 집중하고 있는 RL이나 generative model에 대한 얘기도 좀 더 많이 나오면 좋겠네요 ㅎㅎ 다음에는 이번 ICML에서 best paper를 받은 PixelRNN과 PixelCNN를 공부해 보려고 합니다

###관련 Github
- [tensorflow-딥러닝 논문 구현-By Carpedm20](https://github.com/carpedm20/deep-rl-tensorflow)
- [자료 모음-홍상진](https://github.com/sangjinhong/deep_learning)
- [keras-강화 학습 구현- By matthiasplappert](https://github.com/matthiasplappert/keras-rl)
